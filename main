from cffi.cffi_opcode import CLASS_NAME
from selenium import webdriver
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import time
import tweepy
import pandas as pd
import statistics
import pyautogui

data = []
data_two = []
data_three = []
data_four = []
final_answero = []

def page_one():
  global data
  # Set up a webdriver instance using the Chrome browser
  driver = webdriver.Chrome()

  # Navigate to the staking page
  driver.get('https://staking.chain.link/')

  # Wait for the page to load
  time.sleep(15)

  # Get the fully rendered HTML of the page
  html = driver.page_source

  # Parse the HTML of the staking page
  soup = BeautifulSoup(html, 'html.parser')

  # Find the table element containing the data you want to extract
  table_element = soup.find('table')

  # Check if the table element was found
  if table_element is not None:
    # Extract the data from the table element
    rows = table_element.find_all('tr')
    for row in rows:
      cols = row.find_all('td')
      cols = [ele.text.strip() for ele in cols]
      data.append([ele for ele in cols if ele]) # Get rid of empty values

  # Print the extracted data
  # print(data)

  # Close the webdriver instance
  driver.close()

def page_two():
  global data_two
  # Set up a webdriver instance using the Chrome browser
  driver = webdriver.Chrome()

  # Navigate to the staking page
  driver.get('https://staking.chain.link/')

  # Wait for the page to load
  time.sleep(5)

  # Find all elements matching the XPath expression for the "Next" button
  next_button = driver.find_element(By.XPATH, '/html/body/div[1]/main/div/div[3]/div[3]/button[2]')

  # Click
  next_button.click()

  # Wait for the page to load
  time.sleep(5)

  # Get the fully rendered HTML of the page
  html = driver.page_source

  # Parse the HTML of the staking page
  soup = BeautifulSoup(html, 'html.parser')

  # Find the table element containing the data you want to extract
  table_element = soup.find('table')

  # Check if the table element was found
  if table_element is not None:
    # Extract the data from the table element
    rows = table_element.find_all('tr')
    for row in rows:
      cols = row.find_all('td')
      cols = [ele.text.strip() for ele in cols]
      data_two.append([ele for ele in cols if ele])  # Get rid of empty values

  # Print the extracted data
  # print(data_two)

  # Close the webdriver instance
  driver.close()

def page_three():
  global data_three
  # Set up a webdriver instance using the Chrome browser
  driver = webdriver.Chrome()

  # Navigate to the staking page
  driver.get('https://staking.chain.link/')

  # Wait for the page to load
  time.sleep(5)

  # Find all elements matching the XPath expression for the "Next" button
  next_button = driver.find_element(By.XPATH, '/html/body/div[1]/main/div/div[3]/div[3]/button[2]')

  # Click
  next_button.click()

  # Wait for the page to load
  time.sleep(5)

  # Find all elements matching the XPath expression for the "Next" button
  next_button = driver.find_element(By.XPATH, '/html/body/div[1]/main/div/div[3]/div[3]/button[2]')

  # Click
  next_button.click()

  # Wait for the page to load
  time.sleep(5)

  # Get the fully rendered HTML of the page
  html = driver.page_source

  # Parse the HTML of the staking page
  soup = BeautifulSoup(html, 'html.parser')

  # Find the table element containing the data you want to extract
  table_element = soup.find('table')

  # Check if the table element was found
  if table_element is not None:
    # Extract the data from the table element
    rows = table_element.find_all('tr')
    for row in rows:
      cols = row.find_all('td')
      cols = [ele.text.strip() for ele in cols]
      data_three.append([ele for ele in cols if ele])  # Get rid of empty values

  # Print the extracted data
  # print(data_three)

  # Close the webdriver instance
  driver.close()

def page_four():
  global data_four
  # Set up a webdriver instance using the Chrome browser
  driver = webdriver.Chrome()

  # Navigate to the staking page
  driver.get('https://staking.chain.link/')

  # Wait for the page to load
  time.sleep(5)

  # Find all elements matching the XPath expression for the "Next" button
  next_button = driver.find_element(By.XPATH, '/html/body/div[1]/main/div/div[3]/div[3]/button[2]')

  # Click
  next_button.click()

  # Wait for the page to load
  time.sleep(5)

  # Find all elements matching the XPath expression for the "Next" button
  next_button = driver.find_element(By.XPATH, '/html/body/div[1]/main/div/div[3]/div[3]/button[2]')

  # Click
  next_button.click()

  # Wait for the page to load
  time.sleep(5)

  # Find all elements matching the XPath expression for the "Next" button
  next_button = driver.find_element(By.XPATH, '/html/body/div[1]/main/div/div[3]/div[3]/button[2]')

  # Click
  next_button.click()

  # Wait for the page to load
  time.sleep(5)

  # Get the fully rendered HTML of the page
  html = driver.page_source

  # Parse the HTML of the staking page
  soup = BeautifulSoup(html, 'html.parser')

  # Find the table element containing the data you want to extract
  table_element = soup.find('table')

  # Check if the table element was found
  if table_element is not None:
    # Extract the data from the table element
    rows = table_element.find_all('tr')
    for row in rows:
      cols = row.find_all('td')
      cols = [ele.text.strip() for ele in cols]
      data_four.append([ele for ele in cols if ele])  # Get rid of empty values

  # Print the extracted data
  # print(data_four)

  # Close the webdriver instance
  driver.close()

def main():
  global final_answero
  # Replace these with your own API keys and tokens
  consumer_key = "..."
  consumer_secret = "..."
  access_token = "..."
  access_token_secret = "..."

  # Set up the OAuth1 authentication
  auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret)
  auth.set_access_token(access_token, access_token_secret)

  # Create the API client
  api = tweepy.API(auth)

  # Set up a webdriver instance using the Chrome browser
  driver = webdriver.Chrome()

  # Navigate to the staking page
  driver.get('https://staking.chain.link/')

  # Wait for the page to load
  time.sleep(15)

  # Find all elements matching the XPath expression for the "Next" button
  next_button = driver.find_element(By.XPATH, '/html/body/div[1]/main/div/div[3]/div[3]/button[2]')

  # Click
  next_button.click()


  # Wait for the page to load
  time.sleep(15)

  # Get the fully rendered HTML of the page
  html = driver.page_source

  # Parse the HTML of the staking page and extract the trusted answers
  soup = BeautifulSoup(html, 'html.parser')

  answers = {
    'trusted': '',
    'linkpools': ''
  }

  # Find the body element
  body_element = soup.find('body')

  # Check if the body element was found
  if body_element is not None:
    # Find the div element with the data-reactroot attribute
    div_element = body_element.find('div', attrs={'data-reactroot': ''})

    # Check if the div element was found
    if div_element is not None:
      # Find the main element with the stakingLayout_container__WzwO_ class
      main_element = div_element.find('main', class_='stakingLayout_container__WzwO_')

      # Check if the main element was found
      if main_element is not None:
        # Find the div element with the homeView_container__8zV5Z class
        home_view_element = main_element.find('div', class_='homeView_container__8zV5Z')

        # Check if the home_view element was found
        if home_view_element is not None:
          # Find the div element with the dataFeed
          data_feed_element = main_element.find('div', class_='dataFeedsSecured_container__SEadr card_container__XeKX6')
          if data_feed_element is not None:
            #Find the div element with feed_data__cGKC_
            feed_data_element = main_element.find('div', class_='feed_data__cGKC_')
            if feed_data_element is not None:
              trusted_answer = feed_data_element.find('h4')
              final_answero = trusted_answer.getText().strip()
              final_answer = final_answero.strip('$')
              answers.update({'trusted': final_answer})

  # Find the body element
  second_body_element = soup.find('body')

  # Check if the body element was found
  if second_body_element is not None:
    # Find the div element with the data-reactroot attribute
    div_element = second_body_element.find('div', attrs={'data-reactroot': ''})

    # Check if the div element was found
    if div_element is not None:
      # Find the main element with the stakingLayout_container__WzwO_ class
      main_element = div_element.find('main', class_='stakingLayout_container__WzwO_')

      # Check if the main element was found
      if main_element is not None:
        # Find the div element with the homeView_container__8zV5Z class
        home_view_element = main_element.find('div', class_='homeView_container__8zV5Z')

        # Check if the home_view_element element was found
        if home_view_element is not None:
          # Find the div element with the dataFeedsSecured_container__SEadr class
          data_feeds_secured_element = home_view_element.find('div', class_='dataFeedsSecured_container__SEadr')

          # Check if the data_feeds_secured_element element was found
          if data_feeds_secured_element is not None:
            # Find the div element with the nodeOperators_container__op_sL class
            node_operators_element = data_feeds_secured_element.find('div', class_='nodeOperators_container__op_sL')

            # Check if the node_operators_element element was found
            if node_operators_element is not None:
              # Find the table element
              table_element = node_operators_element.find('table')

              # Check if the table_element element was found
              if table_element is not None:
                # Extract the table element into a list of dictionaries
                table_data = []

                # Find the thead element
                thead_element = table_element.find('thead')

                # Extract the column names from the th elements
                column_names = [th.get_text().strip() for th in thead_element.find_all('th')]

                # Find the tbody element
                tbody_element = table_element.find('tbody')

                # Iterate over the tr elements
                for tr_element in tbody_element.find_all('tr'):
                  # Extract the data from the td elements
                  data = [td.get_text().strip() for td in tr_element.find_all('td')]

                  # Create a dictionary with the column names as the keys and the data as the values
                  row_data = dict(zip(column_names, data))

                  # Append the dictionary to the list
                  table_data.append(row_data)

                # Print the table data
                linkpool = table_data[4]
                print(linkpool)
                linkpools_answer = linkpool['Latest answer']
                print(linkpools_answer)
                linkpools_time = linkpool['Date']
                print(linkpools_time)
                final_linkpool = linkpools_answer.strip('$')
                print(final_linkpool)
                final_answer = answers['trusted']
                answers.update({'linkpools': final_linkpool})
                global streak
                global highscore
                global trusted
                global total
                if answers['trusted'] != answers['linkpools']:
                  streak += 1
                  # Remove the commas from the values
                  trusted_value = float(answers['trusted'].replace(',', ''))
                  linkpools_value = float(answers['linkpools'].replace(',', ''))

                  # Calculate the absolute difference between the two values
                  difference = abs(trusted_value - linkpools_value)
                  formatted_difference = "{:.2f}".format(difference)

                  if total % 5:
                    if "Linkpool" in top_5_variance.index:
                      if difference > highscore:
                        total += 1
                        message = f'The trusted answer for the ETH/USD feed is ${final_answer} and Linkpool provided ${final_linkpool}. \n \nThey were off by ${formatted_difference}. This is a new highscore! Previous: ${highscore}\n \nThey have submitted {trusted} answers out of {total} attempts this session.\n \nThey have submitted {streak} variant answers consecutively.\n \nThey were among the top 5 most variant Oracles for this update.'
                        api.update_status(message)
                        highscore = difference
                      if difference < highscore:
                        total += 1
                        message = f'The trusted answer for the ETH/USD feed is ${final_answer} and Linkpool provided ${final_linkpool}. \n \nThey were off by ${formatted_difference}. \n \nThey have submitted {trusted} trusted answers out of {total} attempts this session. \n \nThe worst variance has been {highscore}.\n \nThey have submitted {trusted} answers out of {total} attempts this session.\n \nThey have submitted {streak} variant answers consecutively\n \nThey were among the top 5 most variant Oracles for this update.'
                        api.update_status(message)
                    if answers['trusted'] == answers['linkpools']:
                      streak = 0
                      trusted += 1
                      total += 1
                      print(f'we have had a rare occurence at {linkpools_time}')
                    else:
                      if difference > highscore:
                        total += 1
                        message = f'The trusted answer for the ETH/USD feed is ${final_answer} and Linkpool provided ${final_linkpool}. \n \nThey were off by ${formatted_difference}. This is a new highscore! Previous: ${highscore}.\n \nThey have submitted {streak} variant answers consecutively. \n \nThey were not among the top 5 most variant Oracles for this update.'
                        api.update_status(message)
                        highscore = difference
                      if difference < highscore:
                        total += 1
                        message = f'The trusted answer for the ETH/USD feed is ${final_answer} and Linkpool provided ${final_linkpool}. \n \nThey were off by ${formatted_difference}. \n \nThey have submitted {streak} variant answers consecutively. \n \nThey were not among the top 5 most variant Oracles for this update.'
                        api.update_status(message)
                    if answers['trusted'] == answers['linkpools']:
                      streak = 0
                      trusted += 1
                      total += 1
                      print(f'we have had a rare occurence at {linkpools_time}')
                  else:
                    if "Linkpool" in top_5_variance.index:
                      if difference > highscore:
                        total += 1
                        message = f'The trusted answer for the ETH/USD feed is ${final_answer} and Linkpool provided ${final_linkpool}. \n \nThey were off by ${formatted_difference}. This is a new highscore! Previous: ${highscore}\n \nThey have submitted {trusted} answers out of {total} attempts this session.\n \nThey have submitted {streak} variant answers consecutively.\n \nThey were among the top 5 most variant Oracles for this update.'
                        api.update_status(message)
                        highscore = difference
                      if difference < highscore:
                        total += 1
                        message = f'The trusted answer for the ETH/USD feed is ${final_answer} and Linkpool provided ${final_linkpool}. \n \nThey were off by ${formatted_difference}. \n \nThey have submitted {trusted} trusted answers out of {total} attempts this session. \n \nThe worst variance has been {highscore}.\n \nThey have submitted {trusted} answers out of {total} attempts this session.\n \nThey have submitted {streak} variant answers consecutively.\n \nThey were among the top 5 most variant Oracles for this update.'
                        api.update_status(message)
                    if answers['trusted'] == answers['linkpools']:
                      streak = 0
                      trusted += 1
                      total += 1
                      print(f'we have had a rare occurence at {linkpools_time}')
                    else:
                      if difference > highscore:
                        total += 1
                        message = f'The trusted answer for the ETH/USD feed is ${final_answer} and Linkpool provided ${final_linkpool}. \n \nThey were off by ${formatted_difference}. This is a new highscore! Previous: ${highscore}.\n \nThey have submitted {streak} variant answers consecutively. \n \nThey were not among the top 5 most variant Oracles for this update.'
                        api.update_status(message)
                        highscore = difference
                      if difference < highscore:
                        total += 1
                        message = f'The trusted answer for the ETH/USD feed is ${final_answer} and Linkpool provided ${final_linkpool}. \n \nThey were off by ${formatted_difference}. \n \nThey have submitted {streak} variant answers consecutively. \n \nThey were not among the top 5 most variant Oracles for this update.'
                        api.update_status(message)
                    if answers['trusted'] == answers['linkpools']:
                      streak = 0
                      trusted += 1
                      total += 1
                      print(f'we have had a rare occurence at {linkpools_time}')


jango = True

streak = 8
highscore = 1.07
trusted = 0
total = 0

while jango == True:
  page_one()
  page_two()
  page_three()
  page_four()
  # Create a dataframe from the first list
  df1 = pd.DataFrame(data)

  # Create a dataframe from the second list
  df2 = pd.DataFrame(data_two)

  # Create a dataframe from the third list
  df3 = pd.DataFrame(data_three)

  # Create a dataframe from the fourth list
  df4 = pd.DataFrame(data_four)

  # Combine the dataframes using the concat function
  df = pd.concat([df1, df2, df3, df4])

  df = df.dropna(how='all')


  def extract_variance(row):
    name = row[0]
    variance = row[1]
    if variance != trusted:
      return (name, variance)


  variance = df.apply(extract_variance, axis=1)

  variance_sorted = variance.sort_values(ascending=False)
  top_5_variance = variance_sorted.head(5)
  main()
  time.sleep(3600)
else:
  print('failed')
